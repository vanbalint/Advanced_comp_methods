setwd("/media/balint/Storage/Tanulas/Advanced_Computational_Methods")
?cut
cut(1:12, 4)
cut(1:12, c(4, 6))
cut(1:12, breaks=c(4, 6))
cut(1:12, breaks=c(4, 6, 8))
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/cTree.R', echo=TRUE)
intervals_1D <- function(noObs, intervals, epsilon = 0.5,
seed = round(runif(1)*100000)) {
set.seed(seed)
# create 1D input points, where each observation is drawn independently
# from the uniform distribution, interval from 0 to 1
X <- runif(noObs)
X <- sort(X)
# create intervals dynamically with the help of parsing expressions,
# and verify which inputs fall into them
condition <- character()
for (i in 1:length(intervals)) {
if (i==1) {
express <- paste0("(X > ", intervals[[i]][1], " & X < ",
intervals[[i]][2],")" )
} else {
express <- paste0(" | (X > ", intervals[[i]][1], " & X < ",
intervals[[i]][2],")" )
}
condition <- paste0(condition, express)
}
withinIntervals <- eval(parse(text = condition))
# create classes
# 1 if within the intervals with probability 0.5 + epsilon,
# 0 if outside of the intervals with probability 0.5 - epsilon
Y <- rep(NA, noObs)
Y[withinIntervals]  <- rbinom(sum(withinIntervals),  1, 0.5 + epsilon)
Y[!withinIntervals] <- rbinom(sum(!withinIntervals), 1, 0.5 - epsilon)
return(list(X=X, Y=Y))
}
# we create a dataset and illustrate it
intervals <- list(c(0.09, 0.17), c(0.24, 0.37))
data <- intervals_1D(50, intervals, 0.4, 12345)
# plotting
library(ggplot2)
ggplot(data=as.data.frame(data), aes(x=X, y=Y, color=factor(Y))) +
geom_point(shape=124, size=4) +
scale_color_manual("Class", values=c("blue", "red")) +
coord_fixed(ratio=1/4) +
theme_bw() +
annotate("rect",
xmin = intervals[[1]][1],
xmax = intervals[[1]][2],
ymin = 0, ymax = 1, alpha = .2) +
annotate("rect",
xmin = intervals[[2]][1],
xmax = intervals[[2]][2],
ymin = 0, ymax = 1, alpha = .2)
```
intervals <- list(c(0.09, 0.17), c(0.24, 0.37))
epsilon <- 0.4
data <- intervals_1D(500, intervals, epsilon, 1234)
Ks <- 1:100
start_time <- Sys.time()
misError <- rep(NA, length(Ks))
for (K in Ks) {
# print(K)
classResults <- simpleClassTree(K, data$X, data$Y)
misError[K]  <- classResults$misError
}
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/cTree.R', echo=TRUE)
setwd("/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5")
spam <- read.table("/SPAM/spambase.data")
spam <- read.table("/Spam/spambase.data")
spam <- read.table("./Spam/spambase.data")
View(spam)
spam <- read.csv("./Spam/spambase.data")
View(spam)
summary(spam)
simpleClassTree(20, spam[,-58], spam[,58])
str(spam)
simpleClassTree(20, as.numeric(spam[,-58]), spam[,58])
simpleClassTree(20, spam[,5], spam[,58])
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
x<-(1:200)
rep(1:3, 5)
y<-rep(c(rep(0,50), rep(1, 50)), 2)
y
simpleClassTree(4, x, y)
x <- seq(0.00001, 0.99999, length.out = 100)
prob <- cbind(x, 1-x)
View(prob)
MissError <- function(prob) {
MissError <- 1 - apply(prob,1,max)
return(MissError)
}
MissError(prob)
MissError(prob[,1])
prob<- c(0.2, 0.4, 0.4)
Gini <- sum(prob*(1-prob))
CrossEntropy <- - sum(prob*log(prob))
library(assertthat)
MissError <- function(prob) {
assert_that(is.integer(prob) & sum(prob)==1)
MissError <- 1-max(prob)
return(MissError)
}
MissError(prob)
sum(prob)
is.integer(prob)
MissError <- function(prob) {
assert_that(is.numeric(prob) & sum(prob)==1)
MissError <- 1-max(prob)
return(MissError)
}
MissError(prob)
CrossEntropy <- function(prob) {
assert_that(is.numeric(prob) & sum(prob)==1)
CrossEntropy <- - sum(prob*log(prob))
return(CrossEntropy)
}
CrossEntropy(prob)
levels(y)
levels(as.factor(y))
classes <- levels(as.factor(y))
noClasses <- length(classes)
y <- as.factor(y)
baba <- c(1, 2, 2, 2, 3, 3)
baba <-as.factor(baba)
table(baba)
max(table(baba))
which(max(table(baba)))
table(baba)[2]
table(baba)[4]
table(baba)[which.max(table(baba))]
d <- rep(NA, 8)
d <- table(baba)[which.max(table(baba))]
d
d <- rep(NA, 8)
d[1:5] <- table(baba)[which.max(table(baba))]
names(table(baba))
d[1:5] <- names(table(baba)[which.max(table(baba))]
)
d[1:5] <- names(table(baba))[which.max(table(baba))]
table(x)
table(y)
for(class in classes){
cat(class)
}
y
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
findThreshold(x,y,ME)
findThreshold(x,y,"ME")
x
potThres<- 0.3
for(i in 1:noClasses){
prob1[i] <- sum(y[x < potThres] == classes[i])/idx
prob2[i] <- sum(y[x > potThres] == classes[i])/(noPoints-idx)
}
idx=30
for(i in 1:noClasses){
prob1[i] <- sum(y[x < potThres] == classes[i])/idx
prob2[i] <- sum(y[x > potThres] == classes[i])/(noPoints-idx)
}
prob1 <- rep(NA, noClasses)
prob2 <- rep(NA, noClasses)
for(i in 1:noClasses){
prob1[i] <- sum(y[x < potThres] == classes[i])/idx
prob2[i] <- sum(y[x > potThres] == classes[i])/(noPoints-idx)
}
noPoints <- length(x)
for(i in 1:noClasses){
prob1[i] <- sum(y[x < potThres] == classes[i])/idx
prob2[i] <- sum(y[x > potThres] == classes[i])/(noPoints-idx)
}
potThres <- mean(x[idx:(idx+1)])
predictedClasses <- rep(NA, noPoints)
predictedClasses[x < potThres] <- names(table(x[x < potThres]))[which.max(table(x[x < potThres]))]
predictedClasses[x > potThres] <- names(table(x[x > potThres]))[which.max(table(x[x > potThres]))]
# loss of this split
sum(predictedClasses != y)
prob1 <- rep(NA, noClasses)
prob2 <- rep(NA, noClasses)
for(i in 1:noClasses){
prob1[i] <- sum(y[x < potThres] == classes[i])/idx
prob2[i] <- sum(y[x > potThres] == classes[i])/(noPoints-idx)
}
if(costFnc == "ME"){
loss <- idx/noPoints*MissError(prob1)+(noPoints-idx)/noPoints*MissError(prob2)
}
loss <- idx/noPoints*MissError(prob1)+(noPoints-idx)/noPoints*MissError(prob2)
i=1
y[x < potThres] == classes[i]
y[x > potThres] == classes[i]
length(y)
x<-1:200
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
findThreshold(x,y,"ME")
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
findThreshold(x,y,"ME")
findThreshold(x,y,"Gini")
findThreshold(x,y,"Entropy")
findThreshold(x,y,"Gini")
findThreshold(x,y,"Entropy")
costFnc="Entropy"
y <- as.factor(y)
classes <- levels(as.factor(y))
noClasses <- length(classes)
noPoints <- length(x)
losses <- rep(NA, noPoints-1)
thresholds <- rep(NA, noPoints-1)
splitLabels <- matrix(NA, ncol=2, nrow=noPoints-1)
# we go sequentially over each point and cut between that point and the
# closest neighbor
for (idx in 1:(noPoints-1)) {
# locate a potential threshold, a split between two points
potThres <- mean(x[idx:(idx+1)])
# check the classification error, when both sides,
# are classified with the most common label
predictedClasses <- rep(NA, noPoints)
predictedClasses[x < potThres] <- names(table(x[x < potThres]))[which.max(table(x[x < potThres]))]
predictedClasses[x > potThres] <- names(table(x[x > potThres]))[which.max(table(x[x > potThres]))]
# loss of this split
prob1 <- rep(NA, noClasses)
prob2 <- rep(NA, noClasses)
for(i in 1:noClasses){
prob1[i] <- sum(y[x < potThres] == classes[i])/idx
prob2[i] <- sum(y[x > potThres] == classes[i])/(noPoints-idx)
}
if(costFnc == "ME"){
loss <- idx/noPoints*MissError(prob1)+(noPoints-idx)/noPoints*MissError(prob2)
}
if(costFnc == "Gini"){
loss <- idx/noPoints*Gini(prob1)+(noPoints-idx)/noPoints*Gini(prob2)
}
if(costFnc == "Entropy"){
loss <- idx/noPoints*CrossEntropy(prob1)+(noPoints-idx)/noPoints*CrossEntropy(prob2)
}
#        misError <- mean(predictedClasses != y)
# recording the accuracy, thresholds and labels of
# the splitted interval
losses[idx] <- loss
thresholds[idx] <- potThres
splitLabels[idx,] <- c(predictedClasses[x < potThres][1],
predictedClasses[x > potThres][1])
}
minLoss <- min(losses)
log(prob1)
prob1*log(prob1)
CrossEntropy(prob1)
losses
log(0)
CrossEntropy(c(0,1))
0-(-Inf)
0*(-Inf)
1*log(1)
prob<-c(0,0,1)
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
CrossEntropy(c(0,1))
findThreshold(x, y, "Entropy")
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
cTree(x,y,5)
predictedClasses[x < potThres] <- names(table(x[x < potThres]))[which.max(table(x[x < potThres]))]
cTree(x,y,5)
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
cTree(x,y,5)
cTree(x,y,2)
cTree(x,y,5, costFnc = "ME")
X<-x
Y<-y
minPoints<-2
boundaries <- c(0, 1)
k=1
intervals <- cut(X, boundaries, include.lowest = TRUE)
noIntervals <- length(levels(intervals))
thresholds <- rep(NA, noIntervals)
errors <- rep(NA, noIntervals)
splitLabels <- matrix(NA, ncol=2, nrow=noIntervals)
iter=1
x <- X[intervals==levels(intervals)[iter]]
y <- Y[intervals==levels(intervals)[iter]]
results <- findThreshold(x, y, costFnc)
x <- X[intervals==levels(intervals)[iter]]
intervals <- cut(X, boundaries, include.lowest = TRUE)
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
cTree(x,y,5, costFnc = "ME")
boundaries <- c(min(X), max(X))
intervals <- cut(X, boundaries, include.lowest = TRUE)
noIntervals <- length(levels(intervals))
thresholds <- rep(NA, noIntervals)
errors <- rep(NA, noIntervals)
splitLabels <- matrix(NA, ncol=2, nrow=noIntervals)
x <- X[intervals==levels(intervals)[iter]]
y <- Y[intervals==levels(intervals)[iter]]
y
results <- findThreshold(x, y, costFnc)
thresholds[iter] <- results$thres
splitLabels[iter,] <- results$labels
# add the potential threshold to our list of boundaries
boundariesHH <- c(boundaries, abs(results$thres))
boundariesHH <- sort(boundariesHH)
# add the signs of the new threshold which indicates what
# is the label of the of newly splitted interval
if (k==1) {
signsHH <- results$labels
} else {
signsHH <- append(signs, results$labels[1],
after=which(boundariesHH==results$thres)-2)
signsHH[which(boundariesHH==results$thres)] <-
results$labels[2]
}
# now we compute predictions with new boundaries based on the
# potential split
predictedClasses <- cut(X, boundariesHH)
levels(predictedClasses) <- signsHH
errors[iter] <- mean(predictedClasses != Y)
signsHH
results$labels
predictedClasses
source('/media/balint/Storage/Tanulas/Advanced_Computational_Methods/PS5/cTree.R', echo=TRUE)
cTree(X,Y,6)
predictedClasses <- cut(X, boundariesHH)
